# â˜¸ï¸ Phase 4 â€“ Kubernetes Chaos & Scalability

Welcome to **Phase 4** of the CI/CD Chaos Workshop â€” where we deploy our Python apps to Kubernetes and learn to handle real-world chaos in production!

This phase covers:

âœ… Kubernetes deployments  
âœ… Auto-scaling with HPA  
âœ… Chaos engineering experiments  
âœ… Monitoring and observability  
âœ… Blue-green deployments

> ğŸ¯ **Goal:** Prove our apps survive chaos in Kubernetes â€” pods crashing, nodes failing, networks partitioning.

---

## ğŸš€ What We're Building

We're deploying our FastAPI Python app to Kubernetes with:

- **Auto-scaling** based on CPU/memory usage
- **Health checks** and readiness probes
- **Chaos experiments** to test resilience
- **Monitoring** with Prometheus and Grafana
- **Blue-green deployments** for zero-downtime updates

> **Chaos Agent says:** "Let's crash some pods and see what happens!"  
> Our mission: Build apps that survive anything.

---

## â˜¸ï¸ Kubernetes Setup

### âœ… Local Development

For local testing, use one of these options:

**Option 1: Docker Desktop Kubernetes**
```bash
# Enable Kubernetes in Docker Desktop
# Settings â†’ Kubernetes â†’ Enable Kubernetes
kubectl cluster-info
```

**Option 2: Minikube**
```bash
# Start Minikube
minikube start
kubectl cluster-info
```

**Option 3: Kind**
```bash
# Create Kind cluster
kind create cluster --name chaos-workshop
kubectl cluster-info
```

---

## ğŸš€ Scenario 1 â€“ Basic Deployment

### âœ… Why It Matters

Kubernetes deployments need proper **health checks** and **resource limits** to survive chaos.

> **Chaos Event:** "Pods keep crashing and restarting!"

---

### âœ… What We'll Do

âœ… Deploy our FastAPI app to Kubernetes  
âœ… Add health checks and readiness probes  
âœ… Set resource limits and requests  
âœ… Monitor pod status

---

### âœ… Deployment YAML

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: chaos-app
spec:
  replicas: 3
  selector:
    matchLabels:
      app: chaos-app
  template:
    metadata:
      labels:
        app: chaos-app
    spec:
      containers:
      - name: chaos-app
        image: chaos-app:latest
        ports:
        - containerPort: 3000
        resources:
          requests:
            memory: "64Mi"
            cpu: "250m"
          limits:
            memory: "128Mi"
            cpu: "500m"
        livenessProbe:
          httpGet:
            path: /health
            port: 3000
          initialDelaySeconds: 30
          periodSeconds: 10
        readinessProbe:
          httpGet:
            path: /ready
            port: 3000
          initialDelaySeconds: 5
          periodSeconds: 5
```

---

### âœ… Service YAML

```yaml
apiVersion: v1
kind: Service
metadata:
  name: chaos-app-service
spec:
  selector:
    app: chaos-app
  ports:
  - port: 80
    targetPort: 3000
  type: LoadBalancer
```

---

## ğŸš€ Scenario 2 â€“ Auto-Scaling

### âœ… Why It Matters

Auto-scaling ensures your app handles traffic spikes and recovers from failures.

> **Chaos Event:** "Traffic spike! Pods can't handle the load!"

---

### âœ… What We'll Do

âœ… Create HorizontalPodAutoscaler (HPA)  
âœ… Test scaling under load  
âœ… Monitor scaling behavior

---

### âœ… HPA YAML

```yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: chaos-app-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: chaos-app
  minReplicas: 2
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80
```

---

## ğŸš€ Scenario 3 â€“ Chaos Engineering

### âœ… Why It Matters

Chaos engineering proves your app's resilience by intentionally causing failures.

> **Chaos Event:** "Let's kill some pods and see what happens!"

---

### âœ… What We'll Do

âœ… Kill random pods  
âœ… Simulate node failures  
âœ… Test network partitions  
âœ… Monitor recovery time

---

### âœ… Chaos Experiments

```python
def test_pod_kill_chaos():
    """Kill random pods and verify recovery"""
    # Get all pods
    pods = kubectl_get_pods("--selector=app=chaos-app")
    
    # Kill a random pod
    random_pod = random.choice(pods)
    kubectl_delete_pod(random_pod)
    
    # Wait for new pod to be ready
    time.sleep(30)
    
    # Verify service is still responding
    response = requests.get("http://localhost/health")
    assert response.status_code == 200
```

---

## ğŸš€ Scenario 4 â€“ Blue-Green Deployment

### âœ… Why It Matters

Blue-green deployments enable zero-downtime updates and instant rollbacks.

> **Chaos Event:** "Deployment failed! Users are seeing errors!"

---

### âœ… What We'll Do

âœ… Deploy new version alongside old  
âœ… Switch traffic gradually  
âœ… Rollback instantly if needed

---

### âœ… Blue-Green Strategy

```yaml
# Blue deployment (current)
apiVersion: apps/v1
kind: Deployment
metadata:
  name: chaos-app-blue
spec:
  replicas: 3
  template:
    metadata:
      labels:
        app: chaos-app
        version: blue
    spec:
      containers:
      - name: chaos-app
        image: chaos-app:v1
        ports:
        - containerPort: 3000

# Green deployment (new)
apiVersion: apps/v1
kind: Deployment
metadata:
  name: chaos-app-green
spec:
  replicas: 0  # Start with 0 replicas
  template:
    metadata:
      labels:
        app: chaos-app
        version: green
    spec:
      containers:
      - name: chaos-app
        image: chaos-app:v2
        ports:
        - containerPort: 3000
```

---

## ğŸ§ª Chaos Testing Scenarios

### âœ… Scenario 1: Pod Crash Chaos

```bash
# Kill random pods
kubectl get pods --selector=app=chaos-app -o name | xargs -I {} kubectl delete {}

# Verify auto-recovery
kubectl get pods --selector=app=chaos-app
```

### âœ… Scenario 2: Node Failure Simulation

```bash
# Drain a node (simulate node failure)
kubectl drain node-1 --force --ignore-daemonsets

# Verify pods reschedule
kubectl get pods --all-namespaces -o wide
```

### âœ… Scenario 3: Resource Exhaustion

```bash
# Create resource pressure
kubectl run stress-test --image=busybox --requests=cpu=1000m,memory=1Gi --limits=cpu=2000m,memory=2Gi --command -- stress --cpu 4 --vm 2 --vm-bytes 1G
```

---

## ğŸ“Š Monitoring & Observability

### âœ… Metrics to Track

- **Pod health:** Ready/NotReady ratio
- **Scaling:** HPA current/target replicas
- **Performance:** Response time, throughput
- **Resources:** CPU/memory utilization

### âœ… Monitoring Setup

```yaml
# Prometheus ServiceMonitor
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: chaos-app-monitor
spec:
  selector:
    matchLabels:
      app: chaos-app
  endpoints:
  - port: metrics
    interval: 30s
```

---

## ğŸ¯ Next Steps

âœ… **Phase 4 Complete:** You now have Kubernetes mastery!  
âœ… **Ready for Phase 5:** [Final Victory Deploy](final.md)  
âœ… **Chaos Agent Status:** Defeated in Kubernetes resilience! ğŸ•¶ï¸

---

## ğŸ“Š Monitoring & Reporting

### âœ… Kubernetes Metrics

- Deployment success rate
- Pod restart count
- Auto-scaling events
- Resource utilization

### âœ… Chaos Metrics

- Recovery time from pod failures
- Service availability during chaos
- Auto-scaling effectiveness

---

**Remember:** Kubernetes is your fortress against chaos. When pods crash, nodes fail, or networks partition, your app should keep running! ğŸ”¥
